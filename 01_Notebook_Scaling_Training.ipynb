{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling your Deep Learning Training\n",
    "\n",
    "Welcome to the third lab of the course. So far, you've learned about stochastic gradient descent and the effects of batch size on training. You've also learned how to scale training from a single GPU to multiple GPUs. You should have seen a considerable speedup that we'll be measuring in more detail.\n",
    "\n",
    "You've seen that as we scale to multiple GPUs, we effectively increase the batch size. From our testing in lab one, we know that increasing the batch size can increase throughput, but it can also affect a network's ability to train effectively. In this lab, you'll be exploring how training is affected when scaling to multiple GPUs. We will start with measuring the effect of multiple GPUs on throughput, in our case images per second. You'll then go on to learn tools and techniques that can help to maintain high accuracy as you scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling of Performance: Images/Second \n",
    "\n",
    "In the first lab, you evaluated images/second as a measurement of performance of the training. We saw that throughput increased with batch size, but only to a certain point. As the batch size increased to saturate the GPU memory, we saw diminishing returns. Now that we are scaling to multiple GPUs, it would make sense that our throughput would increase once again. How do you think throughput will scale with respect to the number of GPUs used in the training?\n",
    "\n",
    "As in Lab 1, we have a callback that measures throughput in images/second after each epoch. Run the cell below __on one GPU__ and record the throughput in the code cell down below. As you might recall from Lab 1, throughput in the first epoch should be ignored because we are paying one-time costs, so choose a throughput from the second or third epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n",
      "100%|█████████████████████████| 26421880/26421880 [00:01<00:00, 13256384.23it/s]\n",
      "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n",
      "100%|█████████████████████████████████| 29515/29515 [00:00<00:00, 327588.09it/s]\n",
      "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "100%|████████████████████████████| 4422102/4422102 [00:00<00:00, 5972192.16it/s]\n",
      "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "100%|█████████████████████████████████| 5148/5148 [00:00<00:00, 39546294.86it/s]\n",
      "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Epoch =  1: Cumulative Time = 95.015, Epoch Time = 95.015, Images/sec = 630.4666748046875, Validation Loss = 0.514, Validation Accuracy = 0.804\n",
      "Epoch =  2: Cumulative Time = 189.968, Epoch Time = 94.952, Images/sec = 630.8854370117188, Validation Loss = 0.407, Validation Accuracy = 0.856\n",
      "Epoch =  3: Cumulative Time = 285.058, Epoch Time = 95.090, Images/sec = 629.9703369140625, Validation Loss = 0.289, Validation Accuracy = 0.899\n",
      "Early stopping after epoch 3\n"
     ]
    }
   ],
   "source": [
    "!python3 fashion_mnist.py --epochs 3 --num-gpus 1 --batch-size 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, alter the training to run __on all GPUs__. You may notice that throughput will fluctuate between epochs. Pick a throughput value that seems closest to the average, or feel free to calculate the average of a few epochs. Remember not to include the first epoch's value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W socket.cpp:601] [c10d] The client socket has failed to connect to [localhost]:9956 (errno: 99 - Cannot assign requested address).\n",
      "[W socket.cpp:601] [c10d] The client socket has failed to connect to [localhost]:9956 (errno: 99 - Cannot assign requested address).\n",
      "Epoch =  1: Cumulative Time = 25.537, Epoch Time = 25.537, Images/sec = 2345.8037109375, Validation Loss = 0.626, Validation Accuracy = 0.747\n",
      "Epoch =  2: Cumulative Time = 50.078, Epoch Time = 24.542, Images/sec = 2434.48046875, Validation Loss = 0.474, Validation Accuracy = 0.826\n",
      "Epoch =  3: Cumulative Time = 74.737, Epoch Time = 24.658, Images/sec = 2429.342529296875, Validation Loss = 0.428, Validation Accuracy = 0.849\n",
      "Epoch =  4: Cumulative Time = 99.422, Epoch Time = 24.685, Images/sec = 2426.751220703125, Validation Loss = 0.355, Validation Accuracy = 0.872\n",
      "Epoch =  5: Cumulative Time = 124.138, Epoch Time = 24.716, Images/sec = 2423.682373046875, Validation Loss = 0.358, Validation Accuracy = 0.870\n",
      "Early stopping after epoch 5\n",
      "Early stopping after epoch 5\n",
      "Early stopping after epoch 5\n",
      "Early stopping after epoch 5\n"
     ]
    }
   ],
   "source": [
    "!python3 fashion_mnist.py --epochs 5 --num-gpus 4 --batch-size 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record the values you obtained in the cell below, and then execute it to see the scaling ratio. Think about the number you get for a minute, and reflect on why, then reveal the block below by clicking on the three dots to see a commentary on the observed results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi GPU speedup factor: 3.850793650793651\n"
     ]
    }
   ],
   "source": [
    "one_gpu_throughput = 630\n",
    "multi_gpu_throughput = 2426\n",
    "\n",
    "print(\"Multi GPU speedup factor: {}\".format(multi_gpu_throughput/one_gpu_throughput))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "You should see scaling at nearly a linear rate. This is a much more promising result than the scaling we saw in Lab 1 when increasing batch size. It stands to reason that each GPU would add a significant boost to the training process. \n",
    "\n",
    "We don't get perfectly linear scaling. A significant component of this is due to communication between the GPUs when updating weights. There can also be other factors, such as waiting for slower GPUs to finish processing before the weights are averaged. But still, this is pretty good.\n",
    "\n",
    "Production DL training at scale is usually benchmarked against the ideal case of linear scaling (N GPUs should be N times faster than 1 GPU). DDP, and the NCCL library, do a good job of maintaining high throughput, but it's worth mentioning that performance is also intricately tied to the hardware in use.\n",
    "\n",
    "As you scale to more GPUs, multi-node training is required, and further hardware considerations are needed to effectively scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Validation Accuracy Over Time\n",
    "\n",
    "As with the first lab, throughput is just one way of measuring performance. It's important to evaluate the training of the network, specifically its ability to effectively make predictions. To show clear comparisons between different trainings, we'll be plotting the validation accuracy of the network over time.\n",
    "\n",
    "To accomplish this you will be saving off validation accuracy, as well as the total elapsed time after each epoch. Each training will create a CSV file, which will be used to plot the trainings against each other. We provide you with a script to plot the training sessions below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the Functionality\n",
    "\n",
    "First, let's make a copy of the training script in case you need it as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp fashion_mnist.py fashion_mnist_original.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, take a look at the two functions in `functions/save_training_data.py`. Follow the steps in `TODO: save data` of `fashion_mnist.py` to appropriately use these functions. You'll need to pass the functions a name for the CSV file, which will be made up of the hyperparameters that you use in your training. If you have trouble and need help, you can find the solution at `solutions/save_training_data.py`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a Training Session\n",
    "\n",
    "After you've implemented the callback, try training on multiple GPUs. We suggest stopping after a certain amount of time (100-200 seconds). You can do this by clicking the stop button in the top bar (or, Kernel > Interrupt in the menu). You can also use the `--target-accuracy` and `--patience` arguments to stop training as you did in Lab 1. The CSV file will save correctly even if you end the training early. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the training_data folder\n",
    "!mkdir training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python3 fashion_mnist.py --num-gpus FIXME --batch-size 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Data\n",
    "\n",
    "If the callback worked, you should have a CSV file with the data in the `training_data` folder. Feel free to navigate to it in the file browser in the pane on the left, where you can open the file. Then look over the code in the cell below. It will plot all the CSV files in the `training_data` folder and use the names in the legend to identify each training run. Once you have a sense of the code, execute the cell to plot your training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# By default we skip the first row, which contains the headers\n",
    "# By skipping 2 rows, you can disregard the first data-point (0,0) to get a closer look\n",
    "def plot_trainings(skiprows=1):\n",
    "    plt.close()\n",
    "    for filename in os.listdir(\"training_data\"):\n",
    "        if filename == \".ipynb_checkpoints\": continue\n",
    "        x, y = np.loadtxt(\"training_data/\" + filename, delimiter=',', unpack=True, skiprows=skiprows)\n",
    "        plt.plot(x,y, label=filename.split('.csv')[0])\n",
    "\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.title('Training Comparison')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_trainings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clearing up the Data\n",
    "\n",
    "As you go through the following exercises in this lab, you may find that the graph will get cluttered and make it hard to compare trainings against each other. When that happens, we suggest that you go into the `training_data` folder and remove files as you see fit. You can clear up everything in the directory with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this if you want to delete all training data\n",
    "!rm -rf training_data/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note that if you run a second training session with all the same hyperparameters (and thus uses the same name for the CSV file) it will overwrite the original training file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusting the Optimizer: Adding Momentum\n",
    "\n",
    "So far, we've modified our training by altering batch size, as well as distributing across multiple GPUs. We'll explore other hyperparameters, such as learning rate later in the lab. In this section, we'll explore another key tool in improving our training process: selecting and tuning the optimizer that we use. \n",
    "\n",
    "As you may recall in Lab 1, the optimizer updates the weights of the network in order to minimize the loss function. In the stochastic gradient descent optimizer that we've used so far, weights are updated based on their gradient with respect to the loss function of a mini-batch. In other words, we determine how altering a weight will affect the loss, and move a small step in the direction that minimizes that loss for the mini-batch. By taking those steps with each back-propagation, we slowly make our way toward a global minimum, decreasing our loss, and increasing our accuracy.\n",
    "\n",
    "Though this process works well, it can be improved upon. One downside is that if the network gets near a local minimum or saddle point, the gradient can be quite small, and the training will slow down in that area. The noise introduced by using minibatches might help the model find its way out, but it might take some time. \n",
    "\n",
    "Additionally, there may be areas where the algorithm keeps taking steps in roughly the same direction. It would be advantageous in those areas if the optimizer helped us take larger steps to move toward the global minimum faster.\n",
    "\n",
    "A good solution to these issues is to use momentum. Instead of the algorithm taking a small independent step each time, adding momentum to our optimizer allows the process to retain a memory of the last several steps. If the weights have been moving in a certain direction on average, momentum will help continue to propel the updates in the same direction. If the training is also fluctuating back and forth, it can smooth out this movement. A decent analogy is a ball rolling down a hill, which will pick up and retain momentum.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "In this case implementing momentum is quite simple, as we do not have to alter the code of the optimizer. SGD algorithms will often accept a momentum parameter, as is the case in the PyTorch implementation.\n",
    "\n",
    "Step 1: Alter the argument parser to accept a momentum argument:\n",
    "\n",
    "```Python\n",
    "parser.add_argument('--momentum', type=float, default=0.9,\n",
    "                    help='SGD momentum')\n",
    "```\n",
    "\n",
    "Step 2: Pass the momentum into the optimizer:\n",
    "\n",
    "```Python\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=args.base_lr, momentum=args.momentum)\n",
    "```\n",
    "\n",
    "Step 3: Update the CSV filename to include the momentum hyperparameter:\n",
    "\n",
    "```Python\n",
    "data_filepath = \"training_data/{}ranks-{}bs-{}lr-{}m.csv\".format(WORLD_SIZE, args.batch_size, args.base_lr, args.momentum)\n",
    "```\n",
    "\n",
    "If you need any help, you can find the solution at `solutions/add_momentum.py`. Navigate to the `TODO: momentum` sections of fashion_mnist.py to make the three changes outlined above.\n",
    "\n",
    "### Training with Momentum\n",
    "\n",
    "Run the training again either for the same amount of time as before, or stopping at a certain accuracy using `--target-accuracy`. After the training completes, execute the plotting script to see the difference between training with and without momentum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python3 fashion_mnist.py --num-gpus 4 --batch-size 32 --momentum .9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trainings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\">"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "output_auto_scroll": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
